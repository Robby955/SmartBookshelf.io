Ideal training would be on say 10,000 spines for which we had associated extracted key words from this would then allow us to learn the extraction algorithm as good as possible.

So the training/test data would be text input (the extraction of the spine text) and then keywords: Authors and Title keywords

For example; text="Deep Learning for Coders
with fastai and PyTorch
Howard
& Gugger
O'REILLY
"

author=['Howard','Gugger']
Title=['Deep Learning for Coders with fastai and PyTorch']

Trained on thousands of such examples could also consider how the characters are spaced in the text as well.


NEXT_PUBLIC_BACKEND_URL=http://localhost:8000/
for running localy




# 2024/07/06
docker build -t gcr.io/smartshelf-426516/smartbookshelf-backend-v1.5 .
docker push gcr.io/smartshelf-426516/smartbookshelf-backend-v1.5
gcloud run deploy smartbookshelf-backend --image gcr.io/smartshelf-426516/smartbookshelf-backend-v1.5 --platform managed --region northamerica-northeast1 --allow-unauthenticated --memory 4Gi --cpu 2 --min-instances 1


To Add:

Multi batch processing, able to do higher resolution images with more than just a few books.


Given the variability and potential inconsistencies in spine data, a combination of several methods would likely provide the best results for matching extracted book information with actual books. Here's a more detailed approach incorporating multiple techniques to handle different patterns and inconsistencies:

Step-by-Step Approach
Preprocess Extracted Text:

Clean and standardize the extracted text (e.g., removing special characters, normalizing case).
Extract potential metadata fields (e.g., title, author, publisher) using NLP techniques or regular expressions.
Initial Candidate Retrieval:

Use an API like Google Books or Open Library to retrieve a broad set of candidate books based on the cleaned and extracted text.
Combine multiple queries if necessary, focusing on different parts of the extracted text (e.g., title and author separately).
Text Similarity Matching:

Apply fuzzy string matching to compare the extracted text with the metadata of candidate books.
Use libraries like fuzzywuzzy or rapidfuzz for this purpose.
Semantic Similarity Matching:

Convert the extracted text and candidate book metadata into embeddings using pre-trained models like BERT or Sentence-BERT.
Calculate cosine similarity between the embeddings to find the closest matches.
Weighted Scoring System:

Develop a scoring system that combines the results of fuzzy matching, semantic similarity, and any additional heuristics (e.g., exact matches on parts of the text, frequency of terms).
Weight the different components based on their importance and reliability.
User Feedback Loop:

Present the top matches to the user and allow them to select the correct book or indicate that none of the matches are correct.
Use this feedback to improve the matching algorithm over time, potentially incorporating machine learning to adjust weights and heuristics.